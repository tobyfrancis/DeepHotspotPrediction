{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Hotspot Prediction and Analysis with Hypercolumns</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import crystallography as xtal\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "from scipy.misc import imrotate\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Reshape, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up our data, sorting in row-major order using some Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/micro9_9.csv')\n",
    "df.sort_values(['x','y','z'],inplace=True)\n",
    "microstructure = df.values.reshape((128,128,128,-1))\n",
    "keys = list(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of available features here, let's see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict(enumerate(keys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first attempt, let's try using only our orientation data. We're going to use 2D slices from our data, with the crystal orientations represented as quaternions (limited to the fundamental one). Let's do the fundamental zone calculations (using Will Lenthe's codebase available at github.com/wlenthe/crystallography), and visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symmetry = xtal.Symmetry('hexagonal')\n",
    "orientations = np.array(microstructure[:,:,:,26:29],dtype='float32')\n",
    "fzqu = np.array(xtal.qu2do(symmetry.fzQu(xtal.eu2qu(orientations))),dtype='float32')\n",
    "hotspots = np.array(microstructure[:,:,:,55],dtype='float32')\n",
    "hotspots[hotspots != 0] = 1\n",
    "grain_ids = np.array(microstructure[:,:,:,36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(8,4))\n",
    "a=fig.add_subplot(1,3,1)\n",
    "plt.imshow(fzqu[0]+128) #increasing the values for visualization\n",
    "a.set_title('Orientations')\n",
    "a=fig.add_subplot(1,3,2)\n",
    "plt.imshow(hotspots[0])\n",
    "a.set_title('Hotspots')\n",
    "a=fig.add_subplot(1,3,3)\n",
    "plt.imshow(grain_ids[0].astype('float32'))\n",
    "a.set_title('Grain Ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_rotated_slice(fzqu,hotspots,vmstress):\n",
    "    slice_index = np.random.randint(0,128)\n",
    "    rotation = np.random.rand()*360\n",
    "    rotated_fzqu = imrotate(fzqu[:,:,slice_index], rotation, interp='nearest')\n",
    "    rotated_hotspots = imrotate(hotspots[:,:,slice_index], rotation, interp='nearest')\n",
    "    rotated_vmstress = imrotate(vmstress[:,:,slice_index], rotation, interp='nearest')\n",
    "    \n",
    "    rotated_hotspots = np.expand_dims(rotated_hotspots,axis=-1)\n",
    "    rotated_vmstress = np.expand_dims(rotated_vmstress,axis=-1)\n",
    "    rotated = np.concatenate([rotated_fzqu,rotated_hotspots,rotated_vmstress],axis=-1)\n",
    "    center = (int(rotated.shape[0]/2),int(rotated.shape[1]/2))\n",
    "    cropped = rotated[center[0]-40:center[0]+40,center[1]-40:center[1]+40]\n",
    "    expanded = np.expand_dims(cropped,axis=0)\n",
    "    \n",
    "    fz,hot,vm = expanded[:,:,:,:4],expanded[:,:,:,4],expanded[:,:,:,5]\n",
    "    hot[hot != 0] = 1\n",
    "    hot = np.expand_dims(hot,axis=-1)\n",
    "    #not_hot = np.array(np.logical_not(hot),dtype='float32')\n",
    "    #hot = np.concatenate((hot,not_hot),axis=-1)\n",
    "    vm = np.expand_dims(vm,axis=-1)\n",
    "    return fz,hot,vm\n",
    "\n",
    "def generate_slice(fzqu,hotspots,grain_ids):\n",
    "    slice_index = np.random.randint(0,128)\n",
    "    return np.expand_dims(fzqu[:,:,slice_index],axis=0),\\\n",
    "           np.expand_dims(hotspots[:,:,slice_index],axis=0),\\\n",
    "           grain_ids[:,:,slice_index]\n",
    "\n",
    "def sample_grains(grain_ids,hotspots):\n",
    "    grains = list(set(grain_ids.flatten()))\n",
    "    #stratified sampling\n",
    "    probs = []\n",
    "    grain = np.random.choice(grains)\n",
    "    indices = np.where(grain_ids[grain_ids==grain].flatten())[0]\n",
    "    indices = np.array(np.unravel_index(indices,grain_ids.shape)).T\n",
    "    indices = np.concatenate((np.zeros((len(indices),1)),indices),axis=-1)\n",
    "    \n",
    "    example = np.where(grain_ids[grain_ids == grain].flatten())[0][0]\n",
    "    hot_or_not = np.array(hotspots.flatten()[example] == 1).reshape(1,1)\n",
    "    return np.expand_dims(indices,axis=0),hot_or_not\n",
    "\n",
    "def generate_batch(fzqu,hotspots,grain_ids):\n",
    "    while True:\n",
    "        fzqu_batch,hotspot_batch,grain_batch = generate_slice(fzqu,hotspots,grain_ids)\n",
    "        indices,hot_or_not = sample_grains(grain_batch,hotspots)\n",
    "        yield [fzqu_batch,indices],hot_or_not\n",
    "        \n",
    "def loss(self, labels, logits):\n",
    "    \"\"\"Adds to the inference model the layers required to generate loss.\"\"\"\n",
    "    with tf.name_scope('loss'):\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            labels = tf.to_int64(labels)\n",
    "            #cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "            \"\"\"Modifying loss function: Ankita Mangal\"\"\"\n",
    "            ratio = tf.reduce_mean(tf.to_float(labels))\n",
    "            temp = tf.to_float(labels)\n",
    "            mapping = lambda x: (1-ratio)*x + (1-x)*(ratio)\n",
    "            class_weight = tf.map_fn(mapping, temp)\n",
    "            labels = tf.to_int64(labels)\n",
    "            #class_weight = tf.constant(ratio)\n",
    "            #weights should be transformed to batchsize\n",
    "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels, logits, weights = class_weight)               \n",
    "            \"\"\"Modification Ends\"\"\"\n",
    "            cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Utility Functions for PixelNet'''\n",
    "def upsample(input_list):\n",
    "    output_list = []\n",
    "    for i,x in enumerate(input_list):\n",
    "        output = K.repeat_elements(K.repeat_elements(x,2**i,axis=1),2**i,axis=2)\n",
    "        output_list.append(output)\n",
    "    return K.concatenate(output_list,axis=3)\n",
    "\n",
    "def upsample_shape(input_shapes):\n",
    "    summation = int(np.sum([shape[3] for shape in input_shapes]))\n",
    "    shape = input_shapes[0]\n",
    "    return (shape[0],shape[1],shape[2],summation)\n",
    "\n",
    "def sparse_upsample(inputs):\n",
    "    fmap_list,locs = inputs[:-1],inputs[-1]\n",
    "    hypercolumn = []\n",
    "    index_factor = tf.convert_to_tensor([[1,1,1],[1,2,2],[1,4,4],\\\n",
    "                                         [1,8,8],[1,16,16],[1,32,32]],dtype=tf.float32)\n",
    "    for i in range(len(fmap_list)):\n",
    "        fmap = fmap_list[i]\n",
    "        coords = tf.divide(locs,index_factor[i])\n",
    "        indices = tf.cast(coords, tf.int32)\n",
    "        hypercolumn.append(tf.gather_nd(fmap, indices))\n",
    "    return array_ops.concat(hypercolumn,-1)\n",
    "\n",
    "def sparse_upsample_shape(input_shapes):\n",
    "    fdim = 0\n",
    "    for shape in input_shapes[:-1]:\n",
    "        fdim += shape[-1]\n",
    "    locs_shape = input_shapes[-1]\n",
    "    return (1,locs_shape[1],fdim)\n",
    "    \n",
    "def im_flatten(x):\n",
    "    return K.reshape(x,K.stack([-1,x.shape[1]*x.shape[2],x.shape[3]]))\n",
    "\n",
    "def im_flatten_shape(shape):\n",
    "    return (shape[0],shape[1]*shape[2],shape[3])\n",
    "\n",
    "def expand_dims(x):\n",
    "    return K.expand_dims(x,axis=0)\n",
    "\n",
    "def expand_dims_shape(shape):\n",
    "    return (1,shape[0],shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orientation_net(optimizer):\n",
    "    inputs = [Input(shape=(128,128,4)),Input(shape=(None,3))]\n",
    "    fmap_list = []\n",
    "    fdim = 4\n",
    "    \n",
    "    X = Conv2D(fdim,(3,3),padding='same')(inputs[0])\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "    fmap_list.append(X)\n",
    "    X = MaxPooling2D((2,2),padding='same')(X)\n",
    "    X = Conv2D(fdim,(3,3),padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "    fmap_list.append(X)\n",
    "    X = MaxPooling2D((2,2),padding='same')(X)\n",
    "    X = Conv2D(fdim,(3,3),padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "    fmap_list.append(X)\n",
    "    X = MaxPooling2D((2,2),padding='same')(X)\n",
    "    X = Conv2D(fdim,(3,3),padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "    fmap_list.append(X)\n",
    "    X = MaxPooling2D((2,2),padding='same')(X)\n",
    "    X = Conv2D(fdim,(3,3),padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "    fmap_list.append(X)\n",
    "    \n",
    "    fmap_list.append(inputs[1])\n",
    "    hypercolumns = Lambda(sparse_upsample,output_shape=sparse_upsample_shape)(fmap_list)\n",
    "    x = GlobalMaxPooling1D()(hypercolumns)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    hotspots = Dense(1,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs,outputs=hotspots)\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [1e-2,1e-5]\n",
    "fig = plt.figure(2)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "with tf.device('/gpu:0'):    \n",
    "    optimizer = Adam(lr=params[0],decay=params[1])\n",
    "    loss = []\n",
    "    epochs = 10000\n",
    "    net = orientation_net(optimizer)\n",
    "    generator = generate_batch(fzqu,hotspots,grain_ids)\n",
    "    epoch_array = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        l = generate_batch(fzqu,hotspots,grain_ids)\n",
    "        inputs,outputs= next(generator)\n",
    "        ce = net.train_on_batch(inputs,outputs)\n",
    "        loss.append(ce)\n",
    "        epoch_array.append(e+1)\n",
    "        \n",
    "        ax.clear()\n",
    "        ax.plot(epoch_array,loss)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(4,figsize=(6,6))\n",
    "im_batch,hot_batch,vm_batch = generate_batch(fzqu,hot50,vmstress)\n",
    "prediction = net.predict(im_batch).reshape(80,80)\n",
    "ax = fig.add_subplot(221)\n",
    "ax.imshow(100*np.sum(im_batch,axis=-1)[0])\n",
    "ax = fig.add_subplot(222)\n",
    "ax.imshow(hot_batch[0,:,:,0])\n",
    "ax = fig.add_subplot(223)\n",
    "ax.imshow(prediction)\n",
    "ax = fig.add_subplot(224)\n",
    "ax.imshow(vm_batch[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_visualization = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,layer in enumerate(net.layers):\n",
    "    if i < 11 and i > 0:\n",
    "        feature_visualization.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
